{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StarcNet demo for NGC1566\n",
    "This notebook executes a demo with **already downloaded** `.fits` and `.tabs` files saved to folders `legus/frc_fits_files` and `legus/tab_files` respectively.\n",
    "\n",
    "**Note:** To install the starcnet environment kernel to Jupyter run in your terminal:\n",
    "```\n",
    "python -m ipykernel install --user --name starcnet --display-name \"starcnet\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Getting started](#Getting-started)\n",
    "* [Create dataset](#Create-dataset)\n",
    "* [Run StarcNet](#Run-StarcNet)\n",
    "    * [Load dataset](#Load-dataset)\n",
    "    * [Classify objects](#Classify-objects)\n",
    "* [Create text file with predictions](#Create-text-file-with-predictions)\n",
    "* [Create galaxy image with predictions](#Create-galaxy-image-with-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "sys.path.insert(0, './src/utils')\n",
    "sys.path.insert(0, './model')\n",
    "from data_utils import load_db\n",
    "from starcnet import Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that galaxy name and filenames are already in the `.txt` files `targets.txt` and `frc_fits_links.txt`. The complete download link should be added if used with online LEGUS catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open('targets.txt', 'r').read())\n",
    "print(open('frc_fits_links.txt', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset\n",
    "Create a 32x32x5 array per object in the catalog (`.tab` file). Each array is object centered and has the 5 bands of the photometric information from the HST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('creating dataset...')\n",
    "os.system('bash create_dataset.sh')\n",
    "print('dataset created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run StarcNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # input batch size for testing (default: 64)\n",
    "data_dir = 'data/' # dataset directory\n",
    "dataset = 'raw_32x32' # dataset file reference\n",
    "checkpoint = 'model/starcnet.pth' # trained model\n",
    "gpu = '' # CUDA visible device (when using a GPU add GPU id (e.g. '0'))\n",
    "cuda = False # enables CUDA training (when using a GPU change to True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all, _, ids = load_db(os.path.join(data_dir,'test_'+dataset+'.dat'))\n",
    "mean = np.load(os.path.join(data_dir,'mean.npy'))\n",
    "\n",
    "data_test = data_all - mean[np.newaxis,:,np.newaxis,np.newaxis] # subtract mean\n",
    "\n",
    "data = torch.from_numpy(data_test).float()\n",
    "test_loader = DataLoader(TensorDataset(data), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader):\n",
    "    '''\n",
    "    Forward pass through the CNN for one epoch\n",
    "    '''\n",
    "    model.eval()\n",
    "    predictions = np.array([], dtype=np.int64).reshape(0) # placeholder for all predictions\n",
    "    scores = np.array([], dtype=np.float32).reshape(0,4) # placeholder for all scores\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            if cuda:\n",
    "                data = Variable(data[0].cuda())\n",
    "            else:\n",
    "                data = Variable(data[0])\n",
    "            output = model(data) # forward pass through the CNN\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            predictions = np.concatenate((predictions, pred.cpu().numpy()))\n",
    "            scores = np.concatenate((scores, output.data.cpu().numpy()),axis=0)\n",
    "    return predictions, scores\n",
    "\n",
    "\n",
    "def load_weights(model):\n",
    "    '''\n",
    "    Load trained parameters to model\n",
    "    '''\n",
    "    model_dict = model.state_dict()\n",
    "    if cuda:\n",
    "        pretrained_dict = torch.load(checkpoint)\n",
    "    else:\n",
    "        pretrained_dict = torch.load(checkpoint, map_location=torch.device('cpu'))\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size() }\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text file with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "cuda = cuda and torch.cuda.is_available()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "model = Net() # create model with StarcNet architecture\n",
    "load_weights(model) # load trained model parameters\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "predictions, scores = test(test_loader) # classify all 32x32x5 arrays\n",
    "\n",
    "np.save(os.path.join('output','scores'), scores) # save scores to 'output/scores.npy' file\n",
    "print('End of classification | time: %.2fs'%(time.time() - start_time))\n",
    "print('-'*40)\n",
    "print('Objects classified as Class 1: %d'%(len(np.where(predictions == 0)[0])))\n",
    "print('Objects classified as Class 2: %d'%(len(np.where(predictions == 1)[0])))\n",
    "print('Objects classified as Class 3: %d'%(len(np.where(predictions == 2)[0])))\n",
    "print('Objects classified as Class 4: %d'%(len(np.where(predictions == 3)[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def L0_1D(x, thresh):\n",
    "\n",
    "    x_abs = np.abs(x)\n",
    "    x_large = np.where(x_abs>thresh)[0]\n",
    "    \n",
    "    y = 0\n",
    "    for i in range(x_large.shape[0]):\n",
    "        if i == 1:\n",
    "            if x_large[i]==1:\n",
    "                y = y + 1\n",
    "        else:\n",
    "            if x_large[i]==1 and x_large[i-1]==0:\n",
    "                y = y + 1\n",
    "    return y\n",
    "\n",
    "def L0_2D(x):\n",
    "    x_abs = np.abs(x)\n",
    "    thresh = np.amax(x_abs)*0.5\n",
    "    y = 0\n",
    "    for i in range(x_abs.shape[0]):\n",
    "        for j in range(x_abs.shape[1]):\n",
    "        \n",
    "            if x_abs[i,j]>thresh:\n",
    "                if i == 0 and j == 0:\n",
    "                    y = y + 1\n",
    "                elif i == 0:\n",
    "                    if x_abs[i,j-1]<=thresh:\n",
    "                        y = y + 1\n",
    "                elif j == 0:\n",
    "                    if x_abs[i-1,j]<=thresh and x_abs[i-1,j+1]<=thresh:\n",
    "                        y = y + 1\n",
    "                elif j == x_abs.shape[1]-1:\n",
    "                    if x_abs[i-1,j]<=thresh and x_abs[i,j-1]<=thresh and x_abs[i-1,j-1]<=thresh:\n",
    "                        y = y + 1                    \n",
    "                else:\n",
    "                    if x_abs[i-1,j]<=thresh and x_abs[i,j-1]<=thresh \\\n",
    "                            and x_abs[i-1,j-1]<=thresh and x_abs[i-1,j+1]<=thresh:\n",
    "                        y = y + 1\n",
    "    return y\n",
    "\n",
    "def entropy1D(X):\n",
    "    e = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        if X[i]>0:\n",
    "            e = e - (X[i] * np.log(X[i]))\n",
    "    return e\n",
    "    \n",
    "def deterministic_dist(n,mesh_v):\n",
    "\n",
    "    targets = np.zeros((n, (mesh_v.shape[0])**n))\n",
    "    \n",
    "    if n==1:\n",
    "        c = 0\n",
    "        for i1 in mesh_v:\n",
    "            targets[:,c]=i1\n",
    "            targets[:,c] = targets[:,c]/norm(targets[:,c],1)\n",
    "            c = c + 1\n",
    "    elif n==2:\n",
    "        c = 0\n",
    "        for i1 in mesh_v:\n",
    "            for i2 in mesh_v:\n",
    "                targets[:,c]=[i1, i2]\n",
    "                targets[:,c] = targets[:,c]/norm(targets[:,c],1)\n",
    "                c = c + 1\n",
    "    elif n==3:\n",
    "        c = 0\n",
    "        for i1 in mesh_v:\n",
    "            for i2 in mesh_v:\n",
    "                for i3 in mesh_v:\n",
    "                    targets[:,c]=[i1, i2, i3]\n",
    "                    targets[:,c] = targets[:,c]/norm(targets[:,c],1)\n",
    "                    c = c + 1\n",
    "    elif n==4:\n",
    "        c = 0\n",
    "        for i1 in mesh_v:\n",
    "            for i2 in mesh_v:\n",
    "                for i3 in mesh_v:\n",
    "                    for i4 in mesh_v:\n",
    "                        targets[:,c]=[i1, i2, i3, i4]\n",
    "                        targets[:,c] = targets[:,c]/norm(targets[:,c],1)\n",
    "                        c = c + 1\n",
    "    elif n==5:\n",
    "        c = 0\n",
    "        for i1 in mesh_v:\n",
    "            for i2 in mesh_v:\n",
    "                for i3 in mesh_v:\n",
    "                    for i4 in mesh_v:\n",
    "                        for i5 in mesh_v:\n",
    "                            targets[:,c]=[i1, i2, i3, i4, i5]\n",
    "                            targets[:,c] = targets[:,c]/norm(targets[:,c],1)\n",
    "                            c = c + 1\n",
    "    else:\n",
    "        for i in range(targets.shape[1]):\n",
    "            ind = np.array(np.unravel_index(i, (mesh_v.shape[0] for j in n)))\n",
    "            for k in range(targets.shape[0]):\n",
    "                targets[k,i] = mesh_v[ind[k]]\n",
    "            targets[:,i] = targets[:,i]/norm(targets[:,i],1)\n",
    "            \n",
    "    return targets\n",
    "\n",
    "def get_rand_peak(image_width, image_height):\n",
    "    (X, Y) = np.meshgrid(np.linspace(-1,1,image_width), np.linspace(-1,1,image_height))\n",
    "    sumXY = np.abs(X)+np.abs(Y)\n",
    "    target_2d = np.maximum(1.- np.random.rand() * sumXY, 0.)\n",
    "    x_shift = np.random.randint(0,image_width)\n",
    "    y_shift = np.random.randint(0,image_height)\n",
    "    target_2d = np.roll(target_2d, x_shift, axis=0)\n",
    "    target_2d = np.roll(target_2d, y_shift, axis=1)\n",
    "    if x_shift<image_width/2.:\n",
    "        target_2d[0:x_shift, :] = 0\n",
    "    else:\n",
    "        target_2d[x_shift:image_width, :] = 0\n",
    "    if y_shift<image_height/2.:\n",
    "        target_2d[:, 0:y_shift] = 0\n",
    "    else:\n",
    "        target_2d[:, y_shift:image_height] = 0\n",
    "    return target_2d\n",
    "\n",
    "def predict_OT(image, lambda_v, channel):\n",
    "\n",
    "    image_width = image.shape[1]\n",
    "    image_height = image.shape[2]\n",
    "    source = image[channel,:,:].flatten()\n",
    "    source = source/norm(source,1)\n",
    "\n",
    "    n = source.shape[0]\n",
    "    epsilon = .01\n",
    "\n",
    "    C = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            (ix, iy) = np.unravel_index(i,(image_width, image_height))\n",
    "            (jx, jy) = np.unravel_index(j,(image_width, image_height))\n",
    "            C[i,j] = norm([ix-jx, iy-jy])\n",
    "\n",
    "    K = np.exp(-C/epsilon)\n",
    "\n",
    "#     mesh_size = 3\n",
    "#     mesh_v = np.linspace(0,1,num=mesh_size)\n",
    "#     targets_size = (mesh_v.shape[0])**n\n",
    "    targets_size = 20\n",
    "    \n",
    "    dist_W = np.zeros((targets_size,lambda_v.shape[0]))\n",
    "    dist_l1 = np.zeros((targets_size,lambda_v.shape[0]))\n",
    "    dist_l2 = np.zeros((targets_size,lambda_v.shape[0]))\n",
    "    target_v = np.zeros((targets_size,lambda_v.shape[0],n))\n",
    "    target_W_entropy = np.zeros((lambda_v.shape[0]))\n",
    "    target_W_points = np.zeros((lambda_v.shape[0]))\n",
    "\n",
    "    for lambda_ind in range(lambda_v.shape[0]):\n",
    "#         targets = deterministic_dist(n,mesh_v)\n",
    "#         for rand_ind in range(targets_size):\n",
    "#             target = targets[:,rand_ind]\n",
    "        for rand_ind in range(targets_size):\n",
    "            print(str(rand_ind)+',', end = '')\n",
    "            \n",
    "            if np.random.rand() > .5:\n",
    "                target_2d = get_rand_peak(image_width, image_height)\n",
    "            else:\n",
    "                target_2d = get_rand_peak(image_width, image_height) \\\n",
    "                            + get_rand_peak(image_width, image_height)\n",
    "                \n",
    "            target = target_2d.flatten()\n",
    "            target = target/norm(target,1)\n",
    "\n",
    "            u = np.ones(n)\n",
    "            v = np.ones(n)\n",
    "            T = np.diag(u) @ K @ np.diag(v)\n",
    "\n",
    "            for opt_ind in range(100):\n",
    "                u = source/(K @ v)\n",
    "                v = target/(K.T @ u)\n",
    "                T = np.diag(u) @ K @ np.diag(v)\n",
    "\n",
    "            dist_W[rand_ind,lambda_ind] = norm(C * T, 'fro') \\\n",
    "                + lambda_v[lambda_ind]*entropy1D(target)\n",
    "            dist_l1[rand_ind,lambda_ind] = norm(source-target,1) \\\n",
    "                + lambda_v[lambda_ind]*entropy1D(target)\n",
    "            dist_l2[rand_ind,lambda_ind] = norm(source-target,2) \\\n",
    "                + lambda_v[lambda_ind]*entropy1D(target)\n",
    "            target_v[rand_ind,lambda_ind,:] = target.flatten()\n",
    "\n",
    "        print('')            \n",
    "        I = np.argmin(dist_W[:,lambda_ind])\n",
    "        optimal_target = target_v[I,lambda_ind,:]\n",
    "        target_W_entropy[lambda_ind] = entropy1D(optimal_target)\n",
    "        target_W_points[lambda_ind] = L0_2D(np.reshape(optimal_target, (image_width, image_height)))\n",
    "    print(target_W_entropy)\n",
    "    print(target_W_points)\n",
    "    return target_W_points[0]\n",
    "    \n",
    "def test_OT(test_loader, predictions_shape, scores_shape, lambda_v, channel, predictions_nnet, \n",
    "            data_subsample, data_np):\n",
    "    prediction_class = 3+np.zeros(predictions_shape, dtype=np.int64)\n",
    "    \n",
    "    for i in data_subsample:\n",
    "        p = predict_OT(data_np[i,:,:,:], lambda_v, channel)\n",
    "        print('data point %d, true class label %d, prediction %f'%(i, 1+predictions_nnet[i], p))\n",
    "\n",
    "        if p==1:\n",
    "            prediction_class[i] = 0\n",
    "        else:\n",
    "            prediction_class[i] = 1\n",
    "\n",
    "    return prediction_class, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('OT:')\n",
    "data_subsample = np.random.choice(predictions.shape[0],size=10,replace=False)\n",
    "data_np = data.numpy()\n",
    "lambda_v = 10.**np.arange(-5,1,1)\n",
    "for lambda_ind in range(lambda_v.shape[0]):\n",
    "    print('lambda: %2.3f'%lambda_v[lambda_ind])\n",
    "    predictions_OT = {}\n",
    "    for i in data_subsample:\n",
    "        if predictions[i]==1: # or predictions[i]==0:\n",
    "            for channel in range(0,5):\n",
    "                plt.matshow(data_np[i,channel,:,:])\n",
    "                plt.title('data point: %d, channel: %d, class: %d'%(i, channel, predictions[i]+1))\n",
    "                print('channel: %d'%channel)\n",
    "                prediction_OT, score_OT = test_OT(test_loader, predictions.shape, scores.shape, \n",
    "                                                  np.array([lambda_v[lambda_ind]]), channel, \n",
    "                                                  predictions, [i], data_np)\n",
    "                print('')\n",
    "                predictions_OT[i] = prediction_OT[i]\n",
    "                \n",
    "    prediction_OT = 0*prediction_OT+3\n",
    "    for k,v in predictions_OT.items():\n",
    "        prediction_OT[k] = v\n",
    "    print('Objects classified as Class 1: %d'%(len(np.where(prediction_OT == 0)[0])))\n",
    "    print('Objects classified as Class 2: %d'%(len(np.where(prediction_OT == 1)[0])))\n",
    "    print('Objects classified as Class 3: %d'%(len(np.where(prediction_OT == 2)[0])))\n",
    "    print('Objects classified as Class 4: %d'%(len(np.where(prediction_OT == 3)[0])))\n",
    "    predictions_agree = np.where(predictions == prediction_OT)\n",
    "    print('Class 1 accuracy: %f'%(len(np.where(predictions[predictions_agree] == 0)[0])/predictions.shape[0]))\n",
    "    print('Class 2 accuracy: %f'%(len(np.where(predictions[predictions_agree] == 1)[0])/predictions.shape[0]))\n",
    "    print('Class 3 accuracy: %f'%(len(np.where(predictions[predictions_agree] == 2)[0])/predictions.shape[0]))\n",
    "    print('Class 4 accuracy: %f'%(len(np.where(predictions[predictions_agree] == 3)[0])/predictions.shape[0]))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_raw_32x32.dat', 'rb') as infile:\n",
    "    dset = pickle.load(infile)\n",
    "data, ids, galaxies, coords = dset['data'], dset['ids'], dset['galaxies'], dset['coordinates']\n",
    "\n",
    "scores = np.load('output/scores.npy')\n",
    "preds = np.argmax(scores,axis=1)\n",
    "\n",
    "with open('output/predictions.csv', 'w') as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['Galaxy', 'Id', 'X', 'Y','Prediction'])\n",
    "    for i in range(len(ids)):\n",
    "        filewriter.writerow([galaxies[i], ids[i], coords[i][0], coords[i][1], preds[i]+1])\n",
    "\n",
    "print(\"Text file with predictions: 'output/predictions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create galaxy image with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('python src/run_visualization.py')\n",
    "\n",
    "print(\"Image with predictions saved to 'output/visualizations/' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize top 10 classified objects per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "\n",
    "def legus2rgb(im):\n",
    "    datac = np.zeros((im.shape[1],im.shape[2],3), dtype=np.float64)\n",
    "    datac[:,:,2] = (21.63*im[0,:,:] + 8.63*im[1,:,:]) / 2.\n",
    "    datac[:,:,1] = (4.52*im[2,:,:])\n",
    "    datac[:,:,0] = (1.23*im[3,:,:] + im[4,:,:]) / 2.\n",
    "    return np.clip(datac, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 16 # patch size (i.e. sz x sz pixels)\n",
    "num_objs = 10 # top-N objects to show\n",
    "\n",
    "for class_label in range(4):\n",
    "    class_ids = np.where(predictions == class_label)\n",
    "    if len(class_ids[0]) == 0: \n",
    "        print('[Class %d] No objects predicted'%(class_label+1))\n",
    "        continue\n",
    "    scores_ids = softmax(scores, axis=1)[class_ids][:,class_label]\n",
    "    sorted_ids = np.argsort(scores_ids)[::-1]\n",
    "    \n",
    "    print('[Class %d] Objects: %d'%(class_label+1,len(sorted_ids)))\n",
    "    fig = plt.figure(figsize=(14, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    for i in range(num_objs):\n",
    "        if i >= len(class_ids[0]): continue\n",
    "        plt.subplot(1,num_objs,i+1)\n",
    "        plt.imshow(legus2rgb(data_all[class_ids][sorted_ids][i,:,:,:]))\n",
    "        plt.title('%d(%.1f%%)'%(ids[class_ids[0][sorted_ids[i]]],scores_ids[sorted_ids[i]]*100))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# Each image has as title the id and score [id(score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
